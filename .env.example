# LLM Provider Configuration
# Supports any OpenAI-compatible API (Mistral, Groq, Together, Gemini, etc.)
LLM_API_KEY=your-api-key-here
LLM_BASE_URL=https://api.groq.com/openai/v1
LLM_MODEL=llama-3.3-70b-versatile

# Optional: separate model for conversation (leave empty to use LLM_MODEL)
# LLM_CONVERSATION_MODEL=

# Legacy Mistral config (still supported if LLM_API_KEY is not set):
# MISTRAL_API_KEY=your-key-here
